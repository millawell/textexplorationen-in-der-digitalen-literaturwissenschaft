{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e75569",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#@title Install dependencies { display-mode: \"form\" }\n",
    "!pip install numpy pandas matplotlib \n",
    "!pip install gensim spacy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f672dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#@title import dependencies and load data { display-mode: \"form\" }\n",
    "DEBUG = True #@param {type:\"boolean\"}\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scripts.utils as utils\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "from spacy.lang.de import German\n",
    "nlp = German()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "preprocessed_data_fn = Path(\"processed_data\") / \"eltec.jsonl\"\n",
    "\n",
    "w2vs = {}\n",
    "vocab = {}\n",
    "embedding_matrix = {}\n",
    "\n",
    "def iter_docs(in_path):\n",
    "    with open(in_path, \"r\") as in_file:\n",
    "        for iline, line in tqdm(enumerate(in_file), desc=\"load documents\"):\n",
    "            data = json.loads(line)\n",
    "            yield data\n",
    "            if iline > 10 and DEBUG:\n",
    "                break\n",
    "                \n",
    "for doc in iter_docs(preprocessed_data_fn):\n",
    "    if len(doc['text']) >= nlp.max_length:\n",
    "        nlp.max_length = len(doc['text'])+1\n",
    "\n",
    "def iter_sents(docs):\n",
    "    for doc in docs:\n",
    "        text = doc['text']\n",
    "        for isent, sent in tqdm(enumerate(nlp(text).sents), desc=\"load sentences\"):\n",
    "            sentence = []\n",
    "            for token in sent:\n",
    "                sentence.append(token.text)\n",
    "            yield {\n",
    "                'text': sentence,\n",
    "                'sentence_id': isent,\n",
    "                **{k:v for k,v in doc.items() if k!='text'}\n",
    "            }\n",
    "            if isent > 100 and DEBUG:\n",
    "                break\n",
    "                \n",
    "def iter_gender(iterable, gender='f'):\n",
    "    if gender not in ['m', 'f']:\n",
    "        raise ValueError('gender must be either \"m\" or \"f\", atm no other genders in the data :/')\n",
    "    for it in iterable:\n",
    "        if it['gender-cat']==gender:\n",
    "             yield it\n",
    "                \n",
    "def iter_author(iterable, author='Willkomm, Ernst Adolf'):\n",
    "    for it in iterable:\n",
    "        if it['author-name']==author:\n",
    "             yield it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fab64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#@title helper functions for w2v { display-mode: \"form\" }\n",
    "class W2VDataset:\n",
    "    def __init__(self, generator_factory):\n",
    "        self.generator_factory = generator_factory\n",
    "        self.length = None\n",
    " \n",
    "    def __iter__(self):\n",
    "        generator = self.generator_factory()\n",
    "        i=0\n",
    "        for i, it in enumerate(generator):\n",
    "            yield it['text']\n",
    "            \n",
    "        self.length = i+1\n",
    "            \n",
    "    def __len__(self):\n",
    "        if self.length is not None:\n",
    "            return self.length\n",
    "        \n",
    "        self.length = len([_ for _ in self])\n",
    "        return self.length\n",
    "        \n",
    "\n",
    "def train_w2v(vector_size=50, window=5, min_count=2, max_vocab_size=20000, epochs=2, split='gender', attribute='f'):\n",
    "    \n",
    "    if split=='gender':\n",
    "        w2vdataset = W2VDataset(\n",
    "            partial(\n",
    "                iter_sents,\n",
    "                (iter_gender(iter_docs(preprocessed_data_fn), gender=attribute))\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    elif split=='author':\n",
    "        \n",
    "        authors = pd.DataFrame(iter_sents(iter_docs(preprocessed_data_fn)))['author-name'].unique()\n",
    "        if attribute not in authors:\n",
    "            raise ValueError('author not in list, check spelling: last name: first name1, first name2')\n",
    "        \n",
    "        w2vdataset = W2VDataset(\n",
    "            partial(\n",
    "                iter_sents,\n",
    "                (iter_author(iter_docs(preprocessed_data_fn), author=attribute))\n",
    "            )\n",
    "        )\n",
    "       \n",
    "    else:\n",
    "        w2vdataset = W2VDataset(\n",
    "            partial(\n",
    "                iter_sents, \n",
    "                iter_docs(preprocessed_data_fn)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    model = Word2Vec(\n",
    "        sentences=w2vdataset,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        workers=4\n",
    "    )\n",
    "    \n",
    "    model.train(\n",
    "        w2vdataset,\n",
    "        total_examples=len(w2vdataset),\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    wv = model.wv\n",
    "\n",
    "    vocab = wv.index_to_key\n",
    "    vocab = sorted(vocab)\n",
    "\n",
    "    vecs = [wv[it] for it in vocab]\n",
    "\n",
    "    return pd.DataFrame(data=vecs, index=vocab)\n",
    "\n",
    "def iter_embed_w2v(vecs, docs):\n",
    "    for doc in docs:\n",
    "        doc_representation = []\n",
    "        for itoken, token in enumerate(doc['text']):\n",
    "            try:\n",
    "                doc_representation.append(vecs.loc[token])\n",
    "            except KeyError:\n",
    "                pass # some tokens are not embedded\n",
    "        yield np.vstack(doc_representation).mean(axis=0)\n",
    "        \n",
    "        \n",
    "def compute_tsne(words, vocab, embedding_matrix):\n",
    "    return words, TSNE(\n",
    "        n_components=2, \n",
    "        learning_rate='auto',\n",
    "        init='random').fit_transform(utils.lookup_embeddings(words, \n",
    "                                                             vocab, \n",
    "                                                             embedding_matrix))\n",
    "\n",
    "def plot_tsne(words, tsne_rep):\n",
    "    plt.scatter(tsne_rep[:,0], tsne_rep[:,1])\n",
    "    for iword, word in enumerate(words):\n",
    "        plt.annotate(word, tsne_rep[iword])\n",
    "        \n",
    "\n",
    "def nearest_neighbors(target, vocab, embedding_matrix, n=25, printout=True):\n",
    "    target_emb = utils.lookup_embeddings([target], vocab, embedding_matrix)\n",
    "    sim_matrix = np.dot(target_emb/np.linalg.norm(target_emb), \n",
    "                        embedding_matrix.T/np.linalg.norm(embedding_matrix, axis=1)[:, None].T)\n",
    "\n",
    "    nearest = [vocab[ii] for ii in np.flip(np.argsort(sim_matrix), axis=1)[0,:n]]\n",
    "    if printout:\n",
    "        print(nearest)\n",
    "            \n",
    "    return nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f655cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title This cell trains word2vec embeddings on the whole corpus.{ display-mode: \"form\" }\n",
    "\n",
    "vector_size=20 #@param {type:\"number\"}\n",
    "window=5 #@param {type:\"number\"}\n",
    "min_count=2 #@param {type:\"number\"}\n",
    "max_vocab_size=20000 #@param {type:\"number\"}\n",
    "epochs=2 #@param {type:\"number\"}\n",
    "split='gender' #@param [\"all\", \"gender\", \"author\", \"book\"]\n",
    "attribute='f' #@param {type:\"string\"}\n",
    "\n",
    "w2vs[attribute] = train_w2v(\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    max_vocab_size=max_vocab_size,\n",
    "    epochs=epochs,\n",
    "    split=split,\n",
    "    attribute=attribute\n",
    ")\n",
    "\n",
    "vocab[attribute] = w2vs[attribute].index\n",
    "embedding_matrix[attribute] = w2vs[attribute].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32abc11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attribute in w2vs.keys():\n",
    "\n",
    "    nearest = nearest_neighbors('Franzose', vocab[attribute], embedding_matrix[attribute], n=10, printout=False)\n",
    "    words, tsne_rep = compute_tsne(nearest, vocab[attribute], embedding_matrix[attribute])\n",
    "    plot_tsne(words, tsne_rep)\n",
    "\n",
    "plt.legend(w2vs.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dhd22",
   "language": "python",
   "name": "dhd22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
