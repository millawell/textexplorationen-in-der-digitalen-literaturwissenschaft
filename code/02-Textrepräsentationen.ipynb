{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6af02bb4",
      "metadata": {
        "id": "6af02bb4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#@title Install dependencies { display-mode: \"form\" }\n",
        "!pip install numpy pandas matplotlib scipy scikit-learn\n",
        "!pip install gensim spacy --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#@title Clone our repository that contains the data { display-mode: \"form\" }\n",
        "!git clone https://github.com/millawell/textexplorationen-in-der-digitalen-literaturwissenschaft\n",
        "%cd textexplorationen-in-der-digitalen-literaturwissenschaft/code/"
      ],
      "metadata": {
        "id": "PKJats9dgfoM"
      },
      "id": "PKJats9dgfoM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "727682ab",
      "metadata": {
        "id": "727682ab"
      },
      "outputs": [],
      "source": [
        "#@title import dependencies and load data { display-mode: \"form\" }\n",
        "#@markdown should a subset of the corpus be used?\n",
        "SUBSET = True #@param {type:\"boolean\"}\n",
        "import json\n",
        "\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "from collections import Counter, defaultdict\n",
        "from bisect import bisect_left\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.sparse import coo_matrix\n",
        "from tqdm.notebook import tqdm\n",
        "from spacy.lang.de import German\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "def iter_docs(in_path):\n",
        "    with open(in_path, \"r\") as in_file:\n",
        "        for iline, line in tqdm(enumerate(in_file), desc=\"load documents\"):\n",
        "            data = json.loads(line)\n",
        "            yield data\n",
        "            if iline > 20 and SUBSET:\n",
        "                break\n",
        "            \n",
        "            \n",
        "def iter_sents(docs):\n",
        "    for doc in docs:\n",
        "        text = doc['text']\n",
        "        for isent, sent in tqdm(enumerate(nlp(text).sents), desc=\"load sentences\"):\n",
        "            sentence = []\n",
        "            for token in sent:\n",
        "                sentence.append(token.text)\n",
        "            yield {\n",
        "                'text': sentence,\n",
        "                'sentence_id': isent,\n",
        "                **{k:v for k,v in doc.items() if k!='text'}\n",
        "            }\n",
        "            if isent > 200 and SUBSET:\n",
        "                break\n",
        "                \n",
        "def iter_chunks(docs, n_tokens=100):\n",
        "    for doc in docs:\n",
        "        tokenized = [t.text for t in nlp(doc['text'])]\n",
        "        for i in range(0, len(tokenized), n_tokens):\n",
        "            yield {\n",
        "                'text': tokenized[i:i + n_tokens],\n",
        "                **{k:v for k,v in doc.items() if k!='text'}\n",
        "            }\n",
        "            if i//n_tokens>100 and SUBSET:\n",
        "                break\n",
        "\n",
        "preprocessed_data_fn = Path(\"processed_data\") / \"eltec.jsonl\"\n",
        "\n",
        "nlp = German()\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "for doc in iter_docs(preprocessed_data_fn):\n",
        "    if len(doc['text']) >= nlp.max_length:\n",
        "        nlp.max_length = len(doc['text'])+1\n",
        "\n",
        "all_data = pd.DataFrame(list(iter_docs(preprocessed_data_fn)))\n",
        "all_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffd577e6",
      "metadata": {
        "id": "ffd577e6"
      },
      "source": [
        "# 1. Representations\n",
        "## One-Hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2594694d",
      "metadata": {
        "id": "2594694d"
      },
      "outputs": [],
      "source": [
        "#@title This cell outputs a one-hot encoding of the first sentence of the first document in the corpus. First, it iterates over the whole corpus to create a vocabulary. As you can see, the output is mostly zeros (*sparse*).{ display-mode: \"form\" }\n",
        "\n",
        "vocab, counts = zip(\n",
        "    *Counter(\n",
        "        [token.text for doc in iter_docs(preprocessed_data_fn) for token in nlp(doc['text'])]\n",
        "    ).most_common(10000)\n",
        ")\n",
        "vocab = sorted(vocab)\n",
        "\n",
        "def get_index(query, vocab):\n",
        "    'Locate the leftmost value exactly equal to x'\n",
        "    i = bisect_left(vocab, query)\n",
        "    if i != len(vocab) and vocab[i] == query:\n",
        "        return i\n",
        "    raise ValueError\n",
        "\n",
        "def iter_embed_1hot(vocab, docs):\n",
        "    for doc in docs:\n",
        "        doc_i = []\n",
        "        doc_j = []\n",
        "        doc_data = []\n",
        "        for itoken, token in enumerate(doc['text']):\n",
        "            try:\n",
        "                token_index = get_index(token, vocab)\n",
        "                doc_i.append(itoken)\n",
        "                doc_j.append(token_index)\n",
        "                doc_data.append(1)\n",
        "            except ValueError:\n",
        "                pass\n",
        "        yield coo_matrix((\n",
        "            np.array(doc_data), \n",
        "            (\n",
        "                np.array(doc_i, dtype=int),\n",
        "                np.array(doc_j, dtype=int)\n",
        "            )\n",
        "        ), shape=(itoken+1, len(vocab)))\n",
        "\n",
        "for embedding in iter_embed_1hot(vocab, iter_sents(iter_docs(preprocessed_data_fn))):\n",
        "    break\n",
        "\n",
        "embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d142084d",
      "metadata": {
        "id": "d142084d"
      },
      "source": [
        "## Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a2623e5",
      "metadata": {
        "id": "1a2623e5"
      },
      "outputs": [],
      "source": [
        "#@title This cell outputs a bag-of-words encoding of the first sentence of the first document in the corpus. Actually, it can be computed by summing over the first axis of the one-hot matrix from above.{ display-mode: \"form\" }\n",
        "\n",
        "def iter_embed_bow(vocab, docs):\n",
        "    for one_hot in iter_embed_1hot(vocab, docs):\n",
        "        yield np.array(one_hot.sum(axis=0)).ravel()\n",
        "\n",
        "for embedding in iter_embed_bow(vocab, iter_sents(iter_docs(preprocessed_data_fn))):\n",
        "    break\n",
        "embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b780644",
      "metadata": {
        "id": "0b780644"
      },
      "source": [
        "## Word 2 Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57a07d84",
      "metadata": {
        "id": "57a07d84"
      },
      "outputs": [],
      "source": [
        "#@title This cell trains word2vec embeddings on the whole corpus. It outputs the average embedding of the first sentence of the first document of the corpus.{ display-mode: \"form\" }\n",
        "\n",
        "vector_size=100 #@param {type:\"number\"}\n",
        "window=5 #@param {type:\"number\"}\n",
        "min_count=1 #@param {type:\"number\"}\n",
        "max_vocab_size=20000 #@param {type:\"number\"}\n",
        "epochs=20 #@param {type:\"number\"}\n",
        "\n",
        "class W2VDataset:\n",
        "    def __init__(self, generator_factory):\n",
        "        self.generator_factory = generator_factory\n",
        "        self.length = None\n",
        " \n",
        "    def __iter__(self):\n",
        "        generator = self.generator_factory()\n",
        "        i=0\n",
        "        for i, it in enumerate(generator):\n",
        "            yield it['text']\n",
        "            \n",
        "        self.length = i+1\n",
        "            \n",
        "    def __len__(self):\n",
        "        if self.length is not None:\n",
        "            return self.length\n",
        "        \n",
        "        self.length = len([_ for _ in self])\n",
        "        return self.length\n",
        "        \n",
        "\n",
        "def train_w2v(vector_size=50, window=5, min_count=2, max_vocab_size=20000, epochs=2):\n",
        "\n",
        "    w2vdataset = W2VDataset(\n",
        "        partial(\n",
        "            iter_sents, \n",
        "            iter_docs(preprocessed_data_fn)\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    model = Word2Vec(\n",
        "        sentences=w2vdataset,\n",
        "        vector_size=vector_size,\n",
        "        window=window,\n",
        "        min_count=min_count,\n",
        "        max_vocab_size=max_vocab_size,\n",
        "        workers=4\n",
        "    )\n",
        "    \n",
        "    model.train(\n",
        "        w2vdataset,\n",
        "        total_examples=len(w2vdataset),\n",
        "        epochs=epochs\n",
        "    )\n",
        "\n",
        "    wv = model.wv\n",
        "\n",
        "    vocab = wv.index_to_key\n",
        "    vocab = sorted(vocab)\n",
        "\n",
        "    vecs = [wv[it] for it in vocab]\n",
        "\n",
        "    return pd.DataFrame(data=vecs, index=vocab)\n",
        "\n",
        "def iter_embed_w2v(vecs, docs):\n",
        "    for doc in docs:\n",
        "        doc_representation = []\n",
        "        for itoken, token in enumerate(doc['text']):\n",
        "            try:\n",
        "                doc_representation.append(vecs.loc[token])\n",
        "            except KeyError:\n",
        "                pass # some tokens are not embedded\n",
        "        yield np.vstack(doc_representation).mean(axis=0)\n",
        "            \n",
        "w2vs = train_w2v(\n",
        "    vector_size=vector_size,\n",
        "    window=window,\n",
        "    min_count=min_count,\n",
        "    max_vocab_size=max_vocab_size,\n",
        "    epochs=epochs\n",
        ")\n",
        "\n",
        "for embedding in iter_embed_w2v(w2vs, iter_sents(iter_docs(preprocessed_data_fn))):\n",
        "    break\n",
        "embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dc7d95f",
      "metadata": {
        "id": "5dc7d95f"
      },
      "source": [
        "# 2. Authorship attribution\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bee3233",
      "metadata": {
        "id": "5bee3233"
      },
      "outputs": [],
      "source": [
        "#@title This cell trains trains a classifier for authorship attribution. It outputs how well the method performs with the given parameters.{ display-mode: \"form\" }\n",
        "\n",
        "n_tokens_per_sample=1000 #@param {type:\"number\"}\n",
        "embedding_method=\"bow\" #@param ['w2v', 'bow']\n",
        "\n",
        "def train_simple_classifier(iterator, embedding_method, label_string):\n",
        "    \n",
        "    data = list(iterator)\n",
        "    labels = [it[label_string] for it in data]\n",
        "    features = np.vstack(list(embedding_method(data)))\n",
        "    \n",
        "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels)\n",
        "\n",
        "    clf = Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"classifier\", LogisticRegression(random_state=13198)),\n",
        "    ])\n",
        "    \n",
        "    \n",
        "    clf = clf.fit(train_features, train_labels)\n",
        "    \n",
        "    \n",
        "    pred_labels = clf.predict(test_features)\n",
        "    \n",
        "    return f1_score(test_labels, pred_labels, average='macro')\n",
        "\n",
        "iterator = iter_chunks(iter_docs(preprocessed_data_fn), n_tokens=n_tokens_per_sample)\n",
        "embedding_method = partial(iter_embed_bow, vocab) if embedding_method == 'bow' else partial(iter_embed_w2v, w2vs)\n",
        "score = train_simple_classifier(\n",
        "  iter_chunks(iter_docs(preprocessed_data_fn)), \n",
        "  embedding_method,\n",
        "  'author-name'\n",
        ")\n",
        "print(f\"classifier reached f1 score of {score} on held-out data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1510607",
      "metadata": {
        "id": "b1510607"
      },
      "source": [
        "# 3. Visualize 3 dimensions with baricentric triangles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72c7ff7e",
      "metadata": {
        "cellView": "form",
        "id": "72c7ff7e"
      },
      "outputs": [],
      "source": [
        "#@title with the baricentric triangle, 3-dimensional data ('weights') can be visualized ([Baryzentrische_Koordinaten auf Wikipedia](https://de.wikipedia.org/wiki/Baryzentrische_Koordinaten)). You can change the values and see where the point is plotted to get a feeling for it.\n",
        "a_axis =  .5#@param {type:\"number\", min:0, max:1, step:0.1}\n",
        "b_axis =  .8#@param {type:\"number\", min:0, max:1, step:0.1}\n",
        "c_axis = 1e-5 #@param {type:\"number\", min:0, max:1, step:0.1}\n",
        "\n",
        "def triangle(coordinates, labels, ax=None, axis_labels=[\"\", \"\", \"\"], colors=None):\n",
        "    \n",
        "    if colors is None:\n",
        "        colors = ['b' for _ in range(len(coordinates))]\n",
        "\n",
        "    def get_cartesian_from_barycentric(b, t):\n",
        "        return t.dot(b)\n",
        "    \n",
        "    \n",
        "    triangle = np.transpose(np.array([[0, 0], [1, 0], [0.5, 0.75**0.5]]))\n",
        "    \n",
        "    cartesian_coordinates = []\n",
        "    \n",
        "    for barycentric_coordinates in coordinates:\n",
        "        if np.abs(barycentric_coordinates).sum() ==0:\n",
        "            barycentric_coordinates = np.ones(3)\n",
        "        barycentric_coordinates = barycentric_coordinates/barycentric_coordinates.sum()\n",
        "        cartesian_coordinates.append(\n",
        "            get_cartesian_from_barycentric(barycentric_coordinates, triangle)\n",
        "        )\n",
        "    \n",
        "    \n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots(figsize=(5,5))\n",
        "    \n",
        "    ax.plot(*triangle.T[:2].T, color=\"black\")\n",
        "    ax.plot(*triangle.T[1:].T, color=\"black\")\n",
        "    ax.plot(*triangle.T[[2,0]].T, color=\"black\")\n",
        "    \n",
        "\n",
        "    for label, vec, color in zip(labels, cartesian_coordinates, colors):\n",
        "        \n",
        "        ax.annotate(label, vec, c=color)\n",
        "        \n",
        "        ax.plot(*vec.T, \"x\", c=color)\n",
        "    \n",
        "    offsets = [(-.04, 0), (.01,0), (-.012,.01)]\n",
        "    for iaxis_label, axis_label in enumerate(axis_labels):\n",
        "        plt.annotate(axis_label, triangle.T[iaxis_label]+np.array(offsets[iaxis_label]))\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.axis('equal')\n",
        "    \n",
        "    return ax\n",
        "\n",
        "triangle(\n",
        "    np.array([[a_axis,b_axis,c_axis]]),\n",
        "    ['test'],\n",
        "    axis_labels=[\"a\", \"b\", \"c\"]\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b60cc636",
      "metadata": {
        "id": "b60cc636"
      },
      "outputs": [],
      "source": [
        "#@title This cell visualizes the corpus data as a baricentric triangle. Two corners can be used for meta-data of the corpus and for the third corner the combined representation of an author can be chosen. All titles will be plotted relative to the distance to this combined author representation. { display-mode: \"form\" }\n",
        "first_corner_category = \"year\" #@param ['Author-birth', 'Author-death', 'year', 'length']\n",
        "second_corner_category = \"length\" #@param ['Author-birth', 'Author-death', 'year', 'length']\n",
        "color_category = \"canon\" #@param ['gender', 'canon']\n",
        "#@markdown ---\n",
        "#@markdown ### 'author representations' corner:\n",
        "third_corner_author = \"Aston, Louise\" #@param str\n",
        "embedding_method=\"bow\" #@param ['w2v', 'bow']\n",
        "metric = \"euclidean\" #@param ['euclidean', 'cosine']\n",
        "\n",
        "docs = list(iter_docs(preprocessed_data_fn))\n",
        "embedding_method = partial(iter_embed_bow, vocab) if embedding_method == 'bow' else partial(iter_embed_w2v, w2vs)\n",
        "\n",
        "author_filter = partial(filter, lambda x: x['author-name'] == third_corner_author)\n",
        "author_representation = np.vstack(list(embedding_method(author_filter(docs))))\n",
        "author_representation.mean(axis=0)\n",
        "title_representations = np.vstack(list(embedding_method(docs)))\n",
        "\n",
        "\n",
        "pw_distances = 1 - np.ravel(pairwise_distances(\n",
        "    author_representation,\n",
        "    title_representations,\n",
        "    metric=metric\n",
        "))\n",
        "\n",
        "\n",
        "def get_label(rec):\n",
        "    author = rec['author-name'][:15].replace(\",\", \"\").replace(\" \", \"-\")\n",
        "    title = rec['book-title'][:10].replace(\",\", \"\").replace(\" \", \"-\")\n",
        "    return f\"{author}_{title}\"\n",
        "\n",
        "def get_color(rec):\n",
        "    if color_category == 'gender':  \n",
        "      return \"y\" if rec['gender-cat'] == 'm' else 'g'\n",
        "    else:\n",
        "      return \"y\" if rec['canon-cat'] == 'high' else 'g'\n",
        "\n",
        "vals = np.array([\n",
        "    (\n",
        "        it[first_corner_category],\n",
        "        it[second_corner_category],\n",
        "        pw_distances[i],\n",
        "        get_label(it),\n",
        "        get_color(it)\n",
        "    )\n",
        "    for i, it in enumerate(docs)\n",
        "])\n",
        "\n",
        "vals, labels, colors = vals[:,:-2].astype(float), vals[:,-2], vals[:,-1]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "\n",
        "vals = vals-vals.min(axis=0)\n",
        "vals = vals/vals.max(axis=0)\n",
        "\n",
        "_ = triangle(\n",
        "    vals,\n",
        "    labels,\n",
        "    axis_labels=[\n",
        "                 first_corner_category,\n",
        "                 second_corner_category,\n",
        "                 third_corner_author\n",
        "    ],\n",
        "    ax=ax,\n",
        "    colors=colors\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6140072b",
      "metadata": {
        "id": "6140072b"
      },
      "source": [
        "## 4. Analogy tests\n",
        "\n",
        "(using word lists from https://github.com/devmount/GermanWordEmbeddings#evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb3e825a",
      "metadata": {
        "id": "eb3e825a"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#@title Load and preprocess German analogy test data.  { display-mode: \"form\" }\n",
        "\n",
        "!git clone https://github.com/devmount/GermanWordEmbeddings.git\n",
        "\n",
        "def queries_from_lists(list_):\n",
        "    queries, answers = [], []\n",
        "    \n",
        "    for row in list_:\n",
        "        \n",
        "        for qw in row:\n",
        "            queries.append(qw)\n",
        "            answers.append([])\n",
        "            for aw in row:\n",
        "                if aw != qw:\n",
        "                    answers[-1].append(aw)\n",
        "                    \n",
        "    return queries, answers\n",
        "\n",
        "\n",
        "def load_syntactic_analogies():\n",
        "    base_path = Path(\"GermanWordEmbeddings/src\")\n",
        "    \n",
        "    adjectives = base_path / \"adjectives.txt\"\n",
        "    adjectives = [it.split(\"-\")  for it in open(adjectives).read().split(\"\\n\")]\n",
        "    \n",
        "    nouns = base_path / \"nouns.txt\"\n",
        "    nouns = [it.split(\"-\")  for it in open(nouns).read().split(\"\\n\")]\n",
        "    \n",
        "    verbs = base_path / \"verbs.txt\"\n",
        "    verbs = [it.split(\"-\")  for it in open(verbs).read().split(\"\\n\")]\n",
        "    \n",
        "    return queries_from_lists(adjectives + nouns + verbs)\n",
        "    \n",
        "\n",
        "def load_semantic_analogies():\n",
        "    base_path = Path(\"GermanWordEmbeddings/src\")\n",
        "    \n",
        "    bestmatch = base_path / \"bestmatch.txt\"\n",
        "    \n",
        "    bestmatch_group = open(bestmatch).read().split(\"\\n: \")\n",
        "\n",
        "    queries = []\n",
        "    answers = []\n",
        "    for group in bestmatch_group:\n",
        "        for pair_i in group.split(\"\\n\")[1:]:\n",
        "            for pair_j in group.split(\"\\n\")[1:]:\n",
        "                if pair_i != pair_j:\n",
        "                  a, b = pair_i.split(\"-\")\n",
        "                  c, d = pair_j.split(\"-\")\n",
        "\n",
        "                  queries.append((a, b, c))\n",
        "                  answers.append(d)    \n",
        "    return queries, answers\n",
        "\n",
        "\n",
        "def evaluate_most_similar(embeddings, queries, answers, n_candidates=20):\n",
        "    \n",
        "    pw_distances = pairwise_distances(embeddings.values, metric='cosine')\n",
        "    pw_distances = pd.DataFrame(\n",
        "        data=pw_distances,\n",
        "        columns=embeddings.index,\n",
        "        index=embeddings.index\n",
        "    )\n",
        "    \n",
        "    num_found_queries = 0\n",
        "    num_total_queries = len(queries)\n",
        "    num_found_answers = 0\n",
        "    num_findable_answers = 0\n",
        "    num_total_answers = sum(map(len, answers))\n",
        "    \n",
        "    for query, answers in zip(queries, answers):\n",
        "        try:\n",
        "            found_row = pw_distances.loc[query]\n",
        "            \n",
        "            candidates = set(\n",
        "                found_row.sort_values()[:n_candidates].index.tolist()\n",
        "            )\n",
        "            answers = set(answers)\n",
        "\n",
        "            num_found_queries += 1\n",
        "            num_findable_answers += len(answers)\n",
        "            num_found_answers += len(candidates & answers)\n",
        "    \n",
        "        except KeyError:\n",
        "            pass\n",
        "        \n",
        "    return {\n",
        "        \"found_queries\": num_found_queries/num_total_queries,\n",
        "        \"found_answers\": num_found_answers/num_findable_answers\n",
        "    }\n",
        "\n",
        "def evaluate_4_analogy(embeddings, queries, answers, n_candidates=20):\n",
        "\n",
        "    num_found_queries = 0\n",
        "    num_total_queries = len(queries)\n",
        "    num_found_answers = 0\n",
        "\n",
        "    for query, answer in zip(queries,  answers):\n",
        "      q1, q2, q3 = query\n",
        "\n",
        "      \n",
        "      try:\n",
        "          query = embeddings.values.dot(\n",
        "              embeddings.loc[q1]\n",
        "              - embeddings.loc[q2]\n",
        "              + embeddings.loc[q3]\n",
        "          )\n",
        "\n",
        "          found_indices = np.argsort(query)[::-1][:n_candidates]\n",
        "\n",
        "          candidates = set(embeddings.index[found_indices])\n",
        "          num_found_queries += 1\n",
        "          num_found_answers += 1 if answer in candidates else 0\n",
        "          if answer in candidates:\n",
        "              print(\"found!\", q1, q2, q3, answer)\n",
        "          \n",
        "      except KeyError:\n",
        "        pass\n",
        "    return {\n",
        "        \"found_queries\": num_found_queries/num_total_queries,\n",
        "        \"found_answers\": num_found_answers/num_found_queries\n",
        "    }\n",
        "\n",
        "\n",
        "syntactic_queries, syntactic_answers = load_syntactic_analogies()\n",
        "semantic_queries, semantic_answers = load_semantic_analogies()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show a few analogy examples.  { display-mode: \"form\" }\n",
        "print(\"Syntactic examples\")\n",
        "for i in range(5):\n",
        "  print(f\"Query: {syntactic_queries[i]}\")\n",
        "  print(f\"Answers: {syntactic_answers[i]}\")\n",
        "\n",
        "print(\"---\")\n",
        "print(\"Semantic examples\")\n",
        "for i in range(5):\n",
        "  print(f\"Query: {semantic_queries[i]}\")\n",
        "  print(f\"Answers: {semantic_answers[i]}\")"
      ],
      "metadata": {
        "id": "0V8UU5T7B9y3"
      },
      "id": "0V8UU5T7B9y3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a307c24",
      "metadata": {
        "scrolled": true,
        "id": "4a307c24"
      },
      "outputs": [],
      "source": [
        "#@title Evaluate w2v embeddings on syntactic and semantic analogies. { display-mode: \"form\" }\n",
        "vector_size=20 #@param {type:\"number\"}\n",
        "window=5 #@param {type:\"number\"}\n",
        "min_count=2 #@param {type:\"number\"}\n",
        "max_vocab_size=1000 #@param {type:\"number\"}\n",
        "epochs=2 #@param {type:\"number\"}\n",
        "\n",
        "embeddings = train_w2v(\n",
        "    vector_size=vector_size,\n",
        "    window=window,\n",
        "    min_count=min_count,\n",
        "    max_vocab_size=max_vocab_size,\n",
        "    epochs=epochs\n",
        ")\n",
        "print(\"syntactic\")\n",
        "print(evaluate_most_similar(embeddings, syntactic_queries, syntactic_answers))\n",
        "\n",
        "print(\"semantic\")\n",
        "print(evaluate_4_analogy(embeddings, semantic_queries, semantic_answers))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (main-env)",
      "language": "python",
      "name": "main-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "name": "Copy of 02-Textrepräsentationen.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}