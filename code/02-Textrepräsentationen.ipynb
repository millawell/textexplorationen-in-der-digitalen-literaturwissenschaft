{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af02bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas matplotlib scipy gensim spacy scikit-learn ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727682ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "from bisect import bisect_left\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import coo_array\n",
    "from tqdm.notebook import tqdm\n",
    "from spacy.lang.de import German\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac22d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_docs(in_path):\n",
    "    with open(in_path, \"r\") as in_file:\n",
    "        for iline, line in tqdm(enumerate(in_file), desc=\"load documents\"):\n",
    "            data = json.loads(line)\n",
    "            yield data\n",
    "            if iline > 10 and DEBUG:\n",
    "                break\n",
    "            \n",
    "            \n",
    "def iter_sents(docs):\n",
    "    for doc in docs:\n",
    "        text = doc['text']\n",
    "        for isent, sent in tqdm(enumerate(nlp(text).sents), desc=\"load sentences\"):\n",
    "            sentence = []\n",
    "            for token in sent:\n",
    "                sentence.append(token.text)\n",
    "            yield {\n",
    "                'text': sentence,\n",
    "                'sentence_id': isent,\n",
    "                **{k:v for k,v in doc.items() if k!='text'}\n",
    "            }\n",
    "            if isent > 100 and DEBUG:\n",
    "                break\n",
    "                \n",
    "def iter_chunks(docs, n_tokens=100):\n",
    "    for doc in docs:\n",
    "        tokenized = [t.text for t in nlp(doc['text'])]\n",
    "        for i in range(0, len(tokenized), n_tokens):\n",
    "            yield {\n",
    "                'text': tokenized[i:i + n_tokens],\n",
    "                **{k:v for k,v in doc.items() if k!='text'}\n",
    "            }\n",
    "            if i//n_tokens>100 and DEBUG:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da49330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_fn = Path(\"processed_data\") / \"eltec.jsonl\"\n",
    "DEBUG = False\n",
    "\n",
    "nlp = German()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "for doc in iter_docs(preprocessed_data_fn):\n",
    "    if len(doc['text']) >= nlp.max_length:\n",
    "        nlp.max_length = len(doc['text'])+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd577e6",
   "metadata": {},
   "source": [
    "## One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2594694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, counts = zip(\n",
    "    *Counter(\n",
    "        [token.text for doc in iter_docs(preprocessed_data_fn) for token in nlp(doc['text'])]\n",
    "    ).most_common(10000)\n",
    ")\n",
    "vocab = sorted(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b78946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(query, vocab):\n",
    "    'Locate the leftmost value exactly equal to x'\n",
    "    i = bisect_left(vocab, query)\n",
    "    if i != len(vocab) and vocab[i] == query:\n",
    "        return i\n",
    "    raise ValueError\n",
    "\n",
    "def iter_embed_1hot(vocab, docs):\n",
    "    for doc in docs:\n",
    "        doc_i = []\n",
    "        doc_j = []\n",
    "        doc_data = []\n",
    "        for itoken, token in enumerate(doc['text']):\n",
    "            try:\n",
    "                token_index = get_index(token, vocab)\n",
    "                doc_i.append(itoken)\n",
    "                doc_j.append(token_index)\n",
    "                doc_data.append(1)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        yield coo_array((\n",
    "            np.array(doc_data), \n",
    "            (\n",
    "                np.array(doc_i, dtype=int),\n",
    "                np.array(doc_j, dtype=int)\n",
    "            )\n",
    "        ), shape=(itoken+1, len(vocab)))\n",
    "\n",
    "for embedding in iter_embed_1hot(vocab, iter_sents(iter_docs(preprocessed_data_fn))):\n",
    "    break\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d142084d",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2623e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_embed_bow(vocab, docs):\n",
    "    for one_hot in iter_embed_1hot(vocab, docs):\n",
    "        yield one_hot.sum(axis=0)\n",
    "\n",
    "for embedding in iter_embed_bow(vocab, iter_sents(iter_docs(preprocessed_data_fn))):\n",
    "    break\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b780644",
   "metadata": {},
   "source": [
    "## Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a07d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2VDataset:\n",
    "    def __init__(self, generator_factory):\n",
    "        self.generator_factory = generator_factory\n",
    "        self.length = None\n",
    " \n",
    "    def __iter__(self):\n",
    "        generator = self.generator_factory()\n",
    "        i=0\n",
    "        for i, it in enumerate(generator):\n",
    "            yield it['text']\n",
    "            \n",
    "        self.length = i+1\n",
    "            \n",
    "    def __len__(self):\n",
    "        if self.length is not None:\n",
    "            return self.length\n",
    "        \n",
    "        self.length = len([_ for _ in self])\n",
    "        return self.length\n",
    "        \n",
    "\n",
    "def train_w2v(vector_size=50, window=5, min_count=2, max_vocab_size=20000, epochs=2):\n",
    "\n",
    "    w2vdataset = W2VDataset(\n",
    "        partial(\n",
    "            iter_sents, \n",
    "            iter_docs(preprocessed_data_fn)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model = Word2Vec(\n",
    "        sentences=w2vdataset,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        workers=4\n",
    "    )\n",
    "\n",
    "    model.train(\n",
    "        w2vdataset,\n",
    "        total_examples=len(w2vdataset),\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    wv = model.wv\n",
    "\n",
    "    vocab = wv.index_to_key\n",
    "    vocab = sorted(vocab)\n",
    "\n",
    "    vecs = [wv[it] for it in vocab]\n",
    "\n",
    "    return pd.DataFrame(data=vecs, index=vocab)\n",
    "\n",
    "def iter_embed_w2v(vecs, docs):\n",
    "    for doc in docs:\n",
    "        doc_representation = []\n",
    "        for itoken, token in enumerate(doc['text']):\n",
    "            try:\n",
    "                doc_representation.append(vecs.loc[token])\n",
    "            except KeyError:\n",
    "                pass # some tokens are not embedded\n",
    "        yield np.vstack(doc_representation).mean(axis=0)\n",
    "            \n",
    "w2vs = train_w2v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde4b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding in iter_embed_w2v(w2vs, iter_sents(iter_docs(preprocessed_data_fn))):\n",
    "    break\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7d95f",
   "metadata": {},
   "source": [
    "## Authorship attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee3233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple_classifier(iterator, label_string):\n",
    "    \n",
    "    data = list(iterator)\n",
    "    labels = [it[label_string] for it in data]\n",
    "    features = np.vstack(list(iter_embed_bow(vocab, data)))\n",
    "    \n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels)\n",
    "\n",
    "    clf = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"classifier\", LogisticRegression(random_state=13198)),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    clf = clf.fit(train_features, train_labels)\n",
    "    \n",
    "    \n",
    "    pred_labels = clf.predict(test_features)\n",
    "    \n",
    "    return f1_score(test_labels, pred_labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5340d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_simple_classifier(iter_chunks(iter_docs(preprocessed_data_fn)), 'author-name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b867794",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "chunk_sizes = [10, 100, 200, 500, 1000]\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    iterator = iter_chunks(iter_docs(preprocessed_data_fn), n_tokens=chunk_size)\n",
    "    scores.append(\n",
    "        train_simple_classifier(iterator, 'author-name')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780519ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(chunk_sizes, scores, label=\"f1_score\")\n",
    "ax.set_xlabel(\"n_tokens\")\n",
    "ax.set_ylabel(\"f1_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1510607",
   "metadata": {},
   "source": [
    "## Visualize 3 dimensions with baricentric triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangle(coordinates, labels, ax=None, axis_labels=[\"\", \"\", \"\"], colors=None):\n",
    "    \n",
    "    if colors is None:\n",
    "        colors = ['b' for _ in range(len(coordinates))]\n",
    "\n",
    "    def get_cartesian_from_barycentric(b, t):\n",
    "        return t.dot(b)\n",
    "    \n",
    "    \n",
    "    triangle = np.transpose(np.array([[0, 0], [1, 0], [0.5, 0.75**0.5]]))\n",
    "    \n",
    "    cartesian_coordinates = []\n",
    "    \n",
    "    for barycentric_coordinates in coordinates:\n",
    "        if np.abs(barycentric_coordinates).sum() ==0:\n",
    "            barycentric_coordinates = np.ones(3)\n",
    "        barycentric_coordinates = barycentric_coordinates/barycentric_coordinates.sum()\n",
    "        cartesian_coordinates.append(\n",
    "            get_cartesian_from_barycentric(barycentric_coordinates, triangle)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(5,5))\n",
    "    \n",
    "    ax.plot(*triangle.T[:2].T, color=\"black\")\n",
    "    ax.plot(*triangle.T[1:].T, color=\"black\")\n",
    "    ax.plot(*triangle.T[[2,0]].T, color=\"black\")\n",
    "    \n",
    "\n",
    "    for label, vec, color in zip(labels, cartesian_coordinates, colors):\n",
    "        \n",
    "        ax.annotate(label, vec, c=color)\n",
    "        \n",
    "        ax.plot(*vec.T, \"x\", c=color)\n",
    "    \n",
    "    offsets = [(-.04, 0), (.01,0), (-.012,.01)]\n",
    "    for iaxis_label, axis_label in enumerate(axis_labels):\n",
    "        plt.annotate(axis_label, triangle.T[iaxis_label]+np.array(offsets[iaxis_label]))\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c7ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title showcase baricentric triangle\n",
    "a_axis = 1/3 #@param {type:\"number\", min:0, max:1, step:0.1}\n",
    "b_axis = 1/5 #@param {type:\"number\", min:0, max:1, step:0.1}\n",
    "c_axis = 1/99 #@param {type:\"number\", min:0, max:1, step:0.1}\n",
    "\n",
    "triangle(\n",
    "    np.array([[a_axis,b_axis,c_axis]]),\n",
    "    ['test'],\n",
    "    axis_labels=[\"a\", \"b\", \"c\"]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60cc636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(rec):\n",
    "    author = rec['author-name'][:15].replace(\",\", \"\").replace(\" \", \"-\")\n",
    "    title = rec['book-title'][:10].replace(\",\", \"\").replace(\" \", \"-\")\n",
    "    return f\"{author}_{title}\"\n",
    "\n",
    "def get_color(rec):\n",
    "    return \"y\" if rec['gender-cat'] == 'm' else 'g'\n",
    "\n",
    "vals = np.array([\n",
    "    (\n",
    "        it['Author-birth'],\n",
    "        it['Author-death'],\n",
    "        it['length'],\n",
    "        get_label(it),\n",
    "        get_color(it)\n",
    "    )\n",
    "    for it in iter_docs(preprocessed_data_fn)\n",
    "])\n",
    "\n",
    "vals, labels, colors = vals[:,:-2].astype(int), vals[:,-2], vals[:,-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "vals = vals-vals.min(axis=0)\n",
    "vals = vals/vals.max(axis=0)\n",
    "\n",
    "_ = triangle(\n",
    "    vals,\n",
    "    ['' for it in labels],\n",
    "    axis_labels=['Author-birth', 'Author-death', \"length\"],\n",
    "    ax=ax,\n",
    "    colors=colors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6140072b",
   "metadata": {},
   "source": [
    "## Analogy tests\n",
    "\n",
    "(using word lists from https://github.com/devmount/GermanWordEmbeddings#evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/devmount/GermanWordEmbeddings.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e5b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queries_from_lists(list_):\n",
    "    queries, answers = [], []\n",
    "    \n",
    "    for row in list_:\n",
    "        \n",
    "        for qw in row:\n",
    "            queries.append(qw)\n",
    "            answers.append([])\n",
    "            for aw in row:\n",
    "                if aw != qw:\n",
    "                    answers[-1].append(aw)\n",
    "                    \n",
    "    return queries, answers\n",
    "\n",
    "\n",
    "def load_syntactic_analogies():\n",
    "    base_path = Path(\"GermanWordEmbeddings/src\")\n",
    "    \n",
    "    adjectives = base_path / \"adjectives.txt\"\n",
    "    adjectives = [it.split(\"-\")  for it in open(adjectives).read().split(\"\\n\")]\n",
    "    \n",
    "    nouns = base_path / \"nouns.txt\"\n",
    "    nouns = [it.split(\"-\")  for it in open(nouns).read().split(\"\\n\")]\n",
    "    \n",
    "    verbs = base_path / \"verbs.txt\"\n",
    "    verbs = [it.split(\"-\")  for it in open(verbs).read().split(\"\\n\")]\n",
    "    \n",
    "    return queries_from_lists(adjectives + nouns + verbs)\n",
    "    \n",
    "\n",
    "def load_semantic_analogies():\n",
    "    base_path = Path(\"GermanWordEmbeddings/src\")\n",
    "    \n",
    "    bestmatch = base_path / \"bestmatch.txt\"\n",
    "    bestmatch = [it.split(\"-\") for it in open(bestmatch).read().split(\"\\n\") if \":\" not in it]\n",
    "    \n",
    "    opposite = base_path / \"opposite.txt\"\n",
    "    opposite = [it.split(\"-\")  for it in open(opposite).read().split(\"\\n\")]\n",
    "    \n",
    "    return queries_from_lists(bestmatch + opposite)\n",
    "\n",
    "\n",
    "def evaluate(embeddings, queries, answers, n_candidates=20):\n",
    "    \n",
    "    pw_distances = pairwise_distances(embeddings.values, metric='cosine')\n",
    "    pw_distances = pd.DataFrame(\n",
    "        data=pw_distances,\n",
    "        columns=embeddings.index,\n",
    "        index=embeddings.index\n",
    "    )\n",
    "    \n",
    "    num_found_queries = 0\n",
    "    num_total_queries = len(queries)\n",
    "    num_found_answers = 0\n",
    "    num_findable_answers = 0\n",
    "    num_total_answers = sum(map(len, answers))\n",
    "    \n",
    "    for query, answers in zip(queries, answers):\n",
    "        try:\n",
    "            found_row = pw_distances.loc[query]\n",
    "            \n",
    "            candidates = set(\n",
    "                found_row.sort_values()[:n_candidates].index.tolist()\n",
    "            )\n",
    "            answers = set(answers)\n",
    "\n",
    "            num_found_queries += 1\n",
    "            num_findable_answers += len(answers)\n",
    "            num_found_answers += len(candidates & answers)\n",
    "    \n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    return {\n",
    "        \"found_queries\": num_found_queries/num_total_queries,\n",
    "        \"found_answers\": num_found_answers/num_findable_answers\n",
    "    }\n",
    "syntactic_queries, syntactic_answers = load_syntactic_analogies()\n",
    "semantic_queries, semantic_answers = load_semantic_analogies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a307c24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings = train_w2v(\n",
    "    vector_size=50,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    max_vocab_size=10000,\n",
    "    epochs=2\n",
    ")\n",
    "print(\"syntactic\")\n",
    "print(evaluate(embeddings, syntactic_queries, syntactic_answers))\n",
    "\n",
    "print(\"semantic\")\n",
    "print(evaluate(embeddings, semantic_queries, semantic_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e60654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = train_w2v(\n",
    "    vector_size=300,\n",
    "    window=7,\n",
    "    min_count=10,\n",
    "    max_vocab_size=20000,\n",
    "    epochs=5\n",
    ")\n",
    "print(\"syntactic\")\n",
    "print(evaluate(embeddings, syntactic_queries, syntactic_answers))\n",
    "\n",
    "print(\"semantic\")\n",
    "print(evaluate(embeddings, semantic_queries, semantic_answers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (main-env)",
   "language": "python",
   "name": "main-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
